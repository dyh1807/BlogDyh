---
title: Miscellaneous Questions
date: 2023-06-20 12:54:38
tags:
---
> 记录各类杂项问题

## Q1

线程和进程的区别？

## A1 (Sys)

进程是一个程序的执行，线程是可被调度执行的最小的单位，一个进程中可能有多个线程。

从OS的角度看，一个CPU核心 core 执行一个 thread；实际上，一个 core 可能可以同时（物理上）执行多个 thread（这样的能力称为 Hyperthreading），但是从OS的视角看，这就是2个 core 在执行 2个 threads。

一个进程和 core 的数量没有直接的关系。一个进程可以具有多个线程，因此一个进程可以在单个CPU工作，也可以在多个CPU工作。对于多进程，如果有多个CPU，则可以每个CPU处理一个进程，达到进程的并发效果；如果只有一个CPU，若仍然需要多进程工作，则需要采用调度算法达到并行效果。

参考:[Christopher F Clark&#39;s Answer](https://www.quora.com/What-is-the-relation-between-a-process-and-a-core-Is-there-a-difference-between-the-core-s-threads-and-process-s-threads)，[Furquan Uddin's Answer](https://qr.ae/pyg4QU)

## Q2 (AI)

什么是预训练？它和正常的训练有什么区别？为什么预训练有助于改善模型的效果？

## A2

我初次了解到预训练来自于VGG。VGG的预训练做法是：先训练一个11层的 NN；训练更深的NN时B，使用A的部分层替换B的部分层。

查看西瓜书和花书，预训练可以分为2阶段: pre-train + fine-tune. 可以采用一种 `greedy layer-wise pretraining` 的方法，逐层训练网络(pre-train)；最后再在整个网络上做训练(fine-tune)。

> 在我看来，预训练的重点是调整了每个参数的训练次序。(虽然训练次数也可能和正常的训练有所调整)

> 此外，2006年 Geoffrey Hinton 的 deep brief network 采用了预训练 `greedy layer-wise pretraining`，被认为是第三次神经网络热潮的突破点。

关于预训练（pretraining）的定义，来自花书：一种策略，在目标模型工作在目标任务之前，在简单模型上做简单的任务。

更加详细的说明：有时候，直接在特定 task 上训练 model 过于野心了(如果模型很复杂，那么这可能很难做到)。

> 因此，预训练是一种优化策略(Optimization Strategies)

除了加快了训练速度以外，实验证明，经过预训练的模型就是能取得比未预训练更好的效果。(在众多的已知任务上)

## Q3 (AI)

ResNet 中的 shortcut 的目的、作用？(来源：AICS-design internship interview)

## A3

> 曾经只做出“保存浅层特征”的回答，被评价为“建议去看一下论文”、“可能懂一点，但没有很懂”。对此，我认为应该从 intuitive 的角度回答。

对于一个深层的网络，其拟合的能力不会比浅层的网络更差，因为即使只在浅层的网络之后加上多层恒等映射直连网络，深层网络的也能达到和浅层网络相同的推理能力。但实际上，作者观察到：当网络的层级加深，深层的网络训练得到的模型效果甚至不如浅层的网络（这称为网络的“退化”）。作者认为这并不是因为深层网络的模型能力不足引发的问题，而是深层模型要获得一个较好的参数是一件比较困难的事情，即“层数加深之后，网络难以训练”。对此，作者参考一种shortcut的连接方法，将浅层直连到深层，这样从数据流向的角度看，等于是增加了多条较浅层的从输入到输出的路径，由此保证网络在训练之后至少具备不会比浅层网络更差的能力。

于我看来，这种对网络的改进是针对“可被训练能力”的改进。

## Q4 (AICS)

AI框架设计中，静态图和动态图有什么差异？

## A4

|     | 静态图 | 动态图 |
| --- | ----- | ------ |
| 差异 | 需要先构建再运行 | 一边运行一边构建|
| 优点 |在运行前可以对图结构进行优化，比如常数折叠、算子融合等，可以获得更快的前向运算速度 | 可以在搭建网络的时候看见变量的值，便于检查 |
| 缺点 | 只有在计算图运行起来之后，才能看到变量的值，像TensorFlow1.x中的session.run那样。 | 前向运算不好优化，因为根本不知道下一步运算要算什么。|

## Q5 (AICS)

AI框架中的一个“算子”指的是什么？

## A5

算子(Operator)指的是一种操作，可以视为一个从输入到输出的映射。卷积、池化等操作均可以被抽象为算子。

算子可能有不同的实现，所谓“高性能算子”就是高性能的算子实现（C++实现、Python实现均不同，寒武纪BCL与BPL、NVIDIA的CUDA均提供算子实现的“智能编程语言”等级描述）。

## Q6 (AI)

Self-Attention中，注意力分数为什么要进行 softmax 操作？为什么要在 softmax 之前除以$d_k$?（$d_k$为 "dimension of k"）

## A6

做 softmax 好理解，就是将一系列系数 $(\alpha_1, \cdots, \alpha_n)$ 映射为 $[0,1]$ 之间，并且他们的和为1，这样就可以在之后表示“各个 value 在输出中的占比”。

之所以除以 $\sqrt{d_k}$ ，从运算的角度看，是因为之后要做softmax，如果输入softmex的值太大，会导致反向求梯度时softmax层的梯度趋近于0（从而导致训练非常缓慢；而如果这个梯度无法在计算机中表示，就等于发生了下溢，梯度实际上变成了0，从而无法训练）；而之所以除的这个值是 $\sqrt{d_k}$ （而不是其它的值），论文中也没有做详细解释，只能认为这是一个经验值。

## QX

To be continue...

## AX

To be continue...
